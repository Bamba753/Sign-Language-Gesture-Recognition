{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Projet Apprentissage supervise\n\n## Langage des signe\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.datasets import * \nimport pandas as pd \n\nimport matplotlib.pyplot as plt \nfrom PIL import ImageFont, ImageDraw, Image\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier \n\nimport cv2\nimport numpy as np\nimport math\nfrom skimage.morphology import skeletonize\nfrom skimage import color,io,img_as_bool,img_as_uint\nfrom skimage import data\nfrom skimage.util import invert\nfrom skimage.filters import threshold_otsu\nimport os\nfrom tqdm import tqdm\n\ncap = cv2.VideoCapture(0)\n\nw = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) \nh = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) \n\n\ncap_region_x_begin = 0.5 \ncap_region_y_end = 0.8  \nthreshold = 60  \nblurValue = 41\nbgSubThreshold = 50\nlearningRate = 0\nbgModel = cv2.createBackgroundSubtractorMOG2(0,bgSubThreshold)\nisCaptured = 1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Fonctions necessaires pour la suite\n\n### removeBackgroud\n    - permet d'eliminer le background de l'image pour une bonne prediction \n    Parametres d'entrees = image\n### findFeatures\n    - permet de determiner les caracteristiques des images dont on va se baser pour creer un modele d'apprentissage\n    Parametres d'entrees = image\n### displayImage\n    - permet d'afficher l'image \n     Parametres d'entrees = image\n### predictClass\n    - permet de predire l'appartenace d'une image a une classe \n    Parametres d'entrees = le modele utilise pendant l'apprentissage supervise\n                            Les differentes classes\n                            L'image"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def removeBackground(frame):\n    fgmask = bgModel.apply(frame, learningRate=learningRate)\n    kernel = np.ones((3, 3), np.uint8)\n    fgmask = cv2.erode(fgmask, kernel, iterations=1)\n    res = cv2.bitwise_and(frame, frame, mask=fgmask)\n    return res"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def findFeatures(frame):\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    blur = cv2.GaussianBlur(gray, (blurValue, blurValue), 0)\n\n    ret, thresh = cv2.threshold(blur, threshold, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU) #seuillage\n    I_skel = cv2.bitwise_not(thresh)\n    image = img_as_bool(I_skel)\n\n    skeleton = skeletonize(image)\n\n    return img_as_uint(skeleton)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def displayImage(img):\n    plt.imshow(img, cmap='gray')\n    plt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def predictClass(model,categories,img):\n    index = model.predict(img)[0]\n    result = categories[index]\n    return result\n    #if np.max(model.predict_proba(img)) >= 0.55 :\n    #    return result\n    #else:\n    #    return \"not found\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Lecture des images et creation de la base de donnees"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": "DATADIR = \"images\" \n\nIMG_SIZE = 100\n\nCATEGORIES = [\"peace\",\"L\",\"wait\",\"dog\"]\nCATEGORIES[1]\nfor category in CATEGORIES:  # Differents signes\n    path = os.path.join(DATADIR,category)  # chemin pour les images\n    for img in os.listdir(path):\n        img_array = cv2.imread(os.path.join(path,img) )  # convertion\n        #img_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  # reduction de la taille de l'image\n        plt.imshow(img_array, cmap='gray') \n        plt.show()\n\n        break  # affichage de la premiere image\n    break"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_training_data():\n    training_data = []\n    IMG_SIZE = 100\n    for category in CATEGORIES:  # Boucle pour tout l alphabet\n\n        path = os.path.join(DATADIR,category)  # chemin pour l alphabet\n        class_num = CATEGORIES.index(category)  # Classification des lettres A = 0 B = 1 C = 2 .....\n\n        for img in tqdm(os.listdir(path)):  # Iteration pour chaque images\n            try:\n                img_array = cv2.imread(os.path.join(path,img),0)  # convert to array\n                #new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  # reduction de la taille de l'image\n                training_data.append([img_array, class_num])  # ajout dans la base de donnees\n                \n            except Exception as e:  # Exception\n                pass\n        \n    return training_data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "data = create_training_data()\nprint(len(data))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import random\nrandom.shuffle(data) # On melange la liste. Initialement les images sont tous dans l'ordre"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "X = []\nY =  []\n\nfor features,labels in data:\n    X.append(features)\n    Y.append(labels)\n\ntrain_x = np.array(X)\ntrain_y = np.array(Y)\nprint(train_x.shape)\nnsamples, nx, ny = train_x.shape\ntrain_x = train_x.reshape((nsamples,nx*ny))\n\nprint(train_x.shape)\nprint(train_y.shape)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Decoupage de la base de donnees en Training set et Testing Set"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "x_train,x_test,y_train,y_test=train_test_split(train_x,train_y,test_size=0.20)\n\nprint(x_train.shape)\nprint(y_train.shape) \nprint(x_train[1])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Construction de notre propre modele:\n### Recherche des parametres du classifieur K-NN\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.model_selection import validation_curve, GridSearchCV\nfrom sklearn import ensemble\nparam_gridkNN = {'n_neighbors' : np.arange(1,5),\n                 'metric' : ['euclidean','minkowski']} \n\ngridkNN = GridSearchCV(KNeighborsClassifier(),param_gridkNN,cv= 5 )\n\ngridkNN.fit(x_train,y_train)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "rfc = ensemble.RandomForestClassifier()\n\nparam_gridrfc = { \n    'n_estimators': [200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}\n\ngridrfc = GridSearchCV(rfc, param_gridrfc, cv= 5)\ngridrfc.fit(x_train, y_train)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(gridkNN.best_score_)\nprint(gridkNN.best_params_)\nkNN = gridkNN.best_estimator_\n\nprint(gridrfc.best_score_)\nprint(gridrfc.best_params_)\nrfc= gridrfc.best_estimator_"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Calcul du Scoring de performane de notre modele"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#la precision par rapport aux donnees de test\nprint(rfc.score(x_test,y_test))\nprint(kNN.score(x_test,y_test))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Test overfitting"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "k = np.arange(4,9)\n\ntrain_score,val_score = validation_curve(KNeighborsClassifier(metric='euclidean'),x_train,y_train,'n_neighbors',k,cv= 5 )\n\nplt.plot(k,train_score.mean(axis = 1), label = 'train')\nplt.plot(k,val_score.mean(axis = 1),label = 'validation')\n\nplt.xlabel('n_neighbors')\nplt.ylabel('score')\nplt.legend()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "k = np.arange(1,10)\n\ntrain_score,val_score = validation_curve(rfc,x_train,y_train,'max_depth',k,cv= 5 )\n\nplt.plot(k,train_score.mean(axis = 1), label = 'train')\nplt.plot(k,val_score.mean(axis = 1),label = 'validation')\n\nplt.xlabel('max_depth')\nplt.ylabel('score')\nplt.legend()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, rfc.predict(x_test))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, kNN.predict(x_test))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Test de prediction de notre modele sur une image faisant pas partie de la base de donnees"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": "import numpy as np\n       \n#Affichage de l'image\nt = cv2.imread(\"19.jpg\",0)\ntest = np.array(t)\ntest1 = test.reshape(1,-1)\ndisplayImage(t)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "result = predictClass(rfc,CATEGORIES,test1)\nprint(result)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Detection et prediction des images en temps reel"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": "while(True):\n    ret,frame = cap.read()\n    frame = cv2.bilateralFilter(frame, 5, 50, 100)  # Lissage de l'image\n    frame = cv2.flip(frame, 1)\n    cv2.imshow('Cam', frame)\n    \n    #mask = bgModel.apply(frame)\n    #kernel = np.ones((3, 3), np.uint8)\n    #mask = cv2.erode(mask, kernel, iterations=1)\n    #frame_mask = cv2.bitwise_and(frame, frame, mask=mask)\n    \n    #cv2.imshow('frame_mask',frame_mask)\n    #frame_back = removeBackground(frame)\n\n    skel = findFeatures(frame)\n    \n    cv2.imshow('skel',skel)   # affichage du squellette\n    \n    skel_reshape = skel.reshape(1,-1)\n\n    result = predictClass(kNN,CATEGORIES,skel_reshape)\n    #print(result)\n    position = (50,50)\n    # Ecriture du resultat au niveau de la frame\n    cv2.putText(\n         frame, \n         result, \n         position,\n         cv2.FONT_HERSHEY_SIMPLEX,\n         1, \n         (255,255, 255, 255), \n         3) \n\n    cv2.imshow('Resultat', frame) # affichage de l'image et le resultat\n    key = cv2.waitKey(1)\n\n    if key & 0xFF == ord('p'):\n        cv2.imwrite(\"1.jpg\",skel)\n        \n        t = cv2.imread(\"1.jpg\",0)\n        \n        skel_load = np.array(t)\n        skel_load_reshape = skel_load.reshape(1,-1)\n        displayImage(skel_load)\n\n\n\n\ncap.release()\ncv2.destroyAllWindows()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
